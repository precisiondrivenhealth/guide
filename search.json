[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "",
    "text": "Welcome\nThis is an online resource intended to help people run successful health data science projects in Aotearoa New Zealand. It has been written and collated based on the experience of data science researchers and other contributors. It is aimed at practitioners and those that oversee their work. The guide is intended to provide practical advice and clarifications and link to publicly available resources and references. In describing data science techniques, the guide assumes a basic technical understanding, addressing the context and principles of how these are applied to lead to a successful outcome.\nWe hope this will be a helpful resource for people and organisations who are starting out in data science, as well as experienced professionals who can share their lessons learned with the wider community in a helpful format.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-this-guide",
    "href": "index.html#about-this-guide",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "About this guide",
    "text": "About this guide\nThe guide is structured roughly along a data science project life cycle: earlier sections outline the context and setup of a health data science project; later sections introduce the technical aspects of data science including modelling.\nThis guide, its website form, and its contents are free to use, licensed under the CC BY-NC-ND 4.0 License.\nPages are Markdown-formatted text files in a public GitHub repository published using Quarto.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contributing-your-mahi",
    "href": "index.html#contributing-your-mahi",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "Contributing your mahi",
    "text": "Contributing your mahi\nPlease contribute your text and ideas to this guide to share your knowledge and questions with the wider health data science community. It is a living document and contributions from all viewpoints are most welcome.\n\nYou are welcome to send suggestions through email to info at precisiondrivenhealth.com. The more specific you can be, the more quickly your suggestions can be incorporated.\nYou can directly contribute clicking the “Edit this page” link in the right sidebar. You’ll be prompted to fork the guide’s repository on GitHub, edit as desired, and can create a pull request to have your edits reviewed and merged into the guide.\nIf you wish to contribute regularly, you can be added as a contributor to the repository.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#development-and-ownership-of-this-guide",
    "href": "index.html#development-and-ownership-of-this-guide",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "Development and ownership of this guide",
    "text": "Development and ownership of this guide\nThis guide was originally created through the Precision Driven Health (PDH) partnership between Aotearoa New Zealand’s health information technology (IT) sector, health providers and universities, aimed at improving health outcomes through data science. PDH sought to increase data science capability in New Zealand’s health sector and encourage innovation in the use of health data. The PDH partnership operated from 2016-2023.\nTe Whatu Ora hosts this guide on behalf of the NZ health system.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contributors-and-reviewers",
    "href": "index.html#contributors-and-reviewers",
    "title": "Health Data Science in Aotearoa New Zealand: A Practical Guide",
    "section": "Contributors and reviewers",
    "text": "Contributors and reviewers\nThanks to the following people who have substantially contributed to conceptualising, writing, and/or reviewing this guide.\n\nAlex Kazemi\nCK Jin\nDuncan Croft\nEdmond Zhang\nFleur Armstrong\nIvan Rivera\nJamal Zolhavarieh\nJuliet Rumball-Smith\nKelly Atkinson\nKevin Ross\nLuke Boyle\nMirza Baig\nNing Hua\nPieta Brown\nQuan Sun\nRachel Owens\nTom Gutteridge\nVipula Dissanayake",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "starting.html",
    "href": "starting.html",
    "title": "1  How do I start?",
    "section": "",
    "text": "No matter how simple your idea, it will involve more than just a spreadsheet or computer code. You’ll need to understand the question to answer, or the clinical concept that you want to address. Getting access to appropriate data to help drive your insights can often be a challenge, too.\nMuch like other aspects of business or research, you’ll probably need to convince at least one other person that your idea is important and worthy of the time or resources you need to pursue it. After navigating these hurdles and producing a model or other analysis, your carefully crafted tool will need care and periodic review to stay functional and relevant.\nIf you are coming from a clinical background, you might appreciate some of the context around understanding data identifiability, and its preparation for modelling.\nIf you are approaching this from a data science background, the unique health data landscape will be important to appreciate.\nIf you are looking for a more general introduction to health data science in general, keep reading from here!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How do I start?</span>"
    ]
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "2  Understanding the basics",
    "section": "",
    "text": "2.1 Data projects have a lifecycle\nExisting data science guides often include the concept of a data science lifecycle. Examples include the CRoss Industry Standard Process for Data Mining (CRISP-DM) and the Microsoft Team Data Science Process (TDSP).\nThis guide aligns broadly with the life cycle stages below, but your project may follow a different path. We’ve also included additional considerations around structuring and resourcing a health data science project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#the-data-science-life-cycle-stages",
    "href": "basics.html#the-data-science-life-cycle-stages",
    "title": "2  Understanding the basics",
    "section": "2.2 The data science life cycle stages",
    "text": "2.2 The data science life cycle stages\n\nBusiness understanding – What does the health system or business or world need?\nData acquisition and understanding – What data do we have/need? Is it ‘clean’?\nData preparation – How do we organise the data for analysis, including modelling?\nModelling – What modelling techniques should we apply?\nEvaluation – Which model best meets the health system or business or world’s objectives?\nDeployment – How do users and stakeholders access the results?\nMaintenance - How will a model be monitored and refreshed over time?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#consider-the-business-case",
    "href": "basics.html#consider-the-business-case",
    "title": "2  Understanding the basics",
    "section": "2.3 Consider the business case",
    "text": "2.3 Consider the business case\n\n2.3.1 Research vs audit vs ‘business as usual’\nPeople and organisations undertaking health data science may need help clarifying whether what they want to do is “research” or not. There are many definitions of research, but a simple definition is:\nResearch creates new knowledge, which could include new methods or processes, and could lead to the creation of new guidelines.\nBusiness as usual (BAU) includes activities such as development, sales, and support. If your business includes the creation of new capabilities or features, this portion of your activities can be perceived as research.\nAudits are activities that check whether you are following existing processes or guidelines. Testing new processes becomes research, since you are departing from the existing processes.\nQuality improvement seeks to improve an existing process that is already of benefit to patients. Machine learning (ML) models incorporated into workflows can cause changes to the standard of care patients receive, including unintended consequences. Translation from a tool to implementation is essentially research to validate and determine the impact of ML on patients.\nResearch projects can have varying goals, such as ‘analysis only’ versus ‘analyse and model for future use’. Some projects may have a commercial focus on delivering an outcome for use in practice. It is important to define what you are doing as clearly as possible at the start of a project (see Social licence and Consent).\n\n\n\n\n\n\nNote\n\n\n\nAll research undertaken in New Zealand using health data requires ethical review (see Ethics & privacy).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#clearly-articulate-the-goal",
    "href": "basics.html#clearly-articulate-the-goal",
    "title": "2  Understanding the basics",
    "section": "2.4 Clearly articulate the goal",
    "text": "2.4 Clearly articulate the goal\nGood data science doesn’t have to be complicated, but it should be clear. Many projects fail to clearly articulate their goal which may lead to people working to a different agenda.\nCo-design - a design-led process that uses creative and participatory methods involving all stakeholders to ensure the result meets their needs - is essential at this early stage. It’s important to be explicit in these circumstances; you cannot define a problem, determine the appropriate data and methodology, interpret results or run a successful project without the input and partnership of the end-users and other stakeholders.\nDefine goals up front, and consider what question is being answered: Do you want to answer a question that is relevant to a specific population (often geographically defined), or to produce a model or other output that can be generalised to other populations? Is it a proof of concept where further work may be required, or does it require implementation to be used in practice?\nOften the real goal is masked by sub-goals. A project that has been forced to fit within a call for proposals or programme, but where the actual goals of the researcher and those that the project is set up to address are different, is one example of this. Carefully consider the problem you’re trying to solve and seek external validation to confirm if it’s a real-world problem for end users.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#consider-equity",
    "href": "basics.html#consider-equity",
    "title": "2  Understanding the basics",
    "section": "2.5 Consider equity",
    "text": "2.5 Consider equity\nEquity (the quality of being fair and impartial) is considered throughout this guide. When setting up a project, plan for this in each step of the process. When this is not considered upfront, overall gains come at the expense of differences in equity.\nA constant view of equity, also called an “equity lens”, is also addressed throughout this guide, and co-design is central to this concept.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "href": "basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "title": "2  Understanding the basics",
    "section": "2.6 Choosing the right question - exploring feasibility and delivering value",
    "text": "2.6 Choosing the right question - exploring feasibility and delivering value\nChoosing the right question is an essential part of any research effort.\nBefore beginning, get really clear on the problem that is being solved and understand if it is clinically meaningful and adds value. Define the need or problem and then find data/technology to answer it, not the other way around. For model development, define what success looks like e.g how accurate should a model be.\n\n\n\n\n\n\nNote\n\n\n\nA human-centred design approach is essential and one type of this is co-design which involves seeking information from different stakeholders (clinicians, patients/consumers, systems users) and research from the existing evidence/base or literature on the topic. Simple frameworks like the Five Whys can be helpful to apply when exploring problems to find a root cause or work arounds. The end-users and those impacted by the use of a tool need to understand and believe in the value being delivered to drive engagement and improve translation into real world applications.\n\n\nMapping the clinical workflow into which a model fits in dynamic contexts can also useful - do clinicians have current workarounds for problems or barriers that remain useful, or is there a genuine gap ? Can the model or algorithm be responsive to real world complexity?\nOften the problem that we start with might originate in the wider health sector and you may need to consider whether there is willingness and need for uptake in this wider context. Another lens for thinking about the problem can be considered throught the value that a solution delivers. Some places where value can be identified is in: improving patient outcomes or experience, improving efficiency, reducing uncertainty, prioritising resource use for those most in need and reducing the use of resource.\nIn building a business case, the value of data science projects needs to be expressed from the perspectives of each of the different stakeholders. Consider the unique selling points, unintended consequences, risks vs benefits from social, economic, health outcomes and reputational perspectives. Articulate value based on each stakeholder’s perspective. Consider engaging with a health economist for specialist advice. Do not exaggerate or over-sell the potential real-life impact or underplay potential risks. (These are discussed more in the Governance section.)\n\n\n\n\n\n\nNote\n\n\n\nWork through the use-case end-to-end before starting, getting clear on how the algorithm will be accessed and used, and what decisions will be made as a result.\n\n\nWhat level of transparency or interpretability (REF transparency section) is required in order for the results to be useful? Do we only care about predictions or are we looking to drive process change?\nConsider how the work will be validated or trialled (REF evaluation section) in a clinical setting, e.g. as a clinical trial or passive background comparison with production data.\nAlgorithms and models are only one piece of the puzzle in improving health outcomes. What are the interventions in place or available to take action based on machine learning-driven insights? For example, if someone is identified as being at higher risk of readmission to hospital,what can be offered as a result?\nIs an algorithm necessarily the best solution? Consider guidance to avoid overuse and misuse of machine learning in clinical research (Volovici et al. 2022).\n\nVolovici, V, Syn, N L, Ercole, A, Zhao, J J, and Liu, N. 2022. “Steps to avoid overuse and misuse of machine learning in clinical research”. Nature Medicine 28.10, 1996–1999. https://www.nature.com/articles/s41591-022-01961-6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#funding",
    "href": "basics.html#funding",
    "title": "2  Understanding the basics",
    "section": "2.7 Funding",
    "text": "2.7 Funding\nFinding funding to pursue a data science project can be difficult. In some cases, data science endeavours may be supported by a specific public or private organisation. Recognising and declaring conflicts of interest is essential to understanding the applicability of the data science results, and any biases that may apply.\nIn New Zealand, funding tends to be available for conceptual research, or for commercialisation activities. In our experience, there is substantial translational work required between these two phases. Potential sources for funding translative work include Callaghan Innovation and the MedTech Innovation Quarter (MedTech-iQ), as well as private companies who invest in research and development activities. Note that the NZ government’s Research and Development Tax Incentive scheme (RDTI) may incentivise these funding activities where the path to commercialisation is less certain.\nIt is usually necessary to pitch, propose, or otherwise justify a request for funding data science activities. You might be responding to a third party’s request for proposals (RFP) or open call for a funding round. When considering the project you want to do, think about the points below.\n\nWho is paying for this work to be completed? What is their interest?\nWho else has an interest in the outcome, including the researchers? Does their influence/interest in the outcome need to be declared or managed in any particular way?\nIf this work is successful, what further investment will be required to ensure that the work leads to its full potential?\nIf the ultimate goal is a commercial product, there is often substantial investment required to take a proof of concept to a maintainable, robust product. Think about:\n\nWho will provide support to the people using this product, even if they are using it only for research purposes or in informal ways?\nWhat is the lifecycle of the research output? Who will be able to look after it in 5 years’ time?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#legal-ip-and-regulatory-considerations",
    "href": "basics.html#legal-ip-and-regulatory-considerations",
    "title": "2  Understanding the basics",
    "section": "2.8 Legal, IP, and regulatory considerations",
    "text": "2.8 Legal, IP, and regulatory considerations\nLegal advice should be sought, and issues related to legal, intellectual property and/or regulatory issues explicitly discussed and documented prior to you starting work. Consider who holds the data you want to use, who owns the models and what may be required for lifecycle management.\nSoftware may be considered a medical device if it is used in the diagnosis, treatment, prevention, cure, or mitigation of diseases or other conditions. Depending on how the intended use is defined, the software may be subject to country dependent regulatory requirements. Specialised regulatory advisors should be engaged early as the process takes time and documentation for approval may need to be generated during the development process.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "basics.html#useful-resources",
    "href": "basics.html#useful-resources",
    "title": "2  Understanding the basics",
    "section": "2.9 Useful resources",
    "text": "2.9 Useful resources\n\nReport on Artificial Intelligence for Health in New Zealand: Hauora i te Atamai Iahiko (AI Forum 2019)\nA UK guide to good practice for digital and data-driven health technologies\nAn Assessment Report on Algorithms published by the New Zealand government (Statistics NZ 2018)\nResearch and engagement considerations described by the NZ Digital Identity Programme\n\n\n\n\nAI Forum. 2019. “Artificial intelligence for health in new zealand: Hauora i te atamai iahiko”. https://aiforum.org.nz/wp-content/uploads/2019/10/AI-For-Health-in-New-Zealand.pdf.\n\nStatistics NZ. 2018. “Algorithm assessment report”. https://www.data.govt.nz/docs/algorithm-assessment-report/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Understanding the basics</span>"
    ]
  },
  {
    "objectID": "collaboration.html",
    "href": "collaboration.html",
    "title": "3  People, capability & collaboration",
    "section": "",
    "text": "3.1 Team capability\nTypical health data science projects require a multidisciplinary team. The involvement of different roles will likely fluctuate through the different stages of a project lifecycle.\nSome individuals will have multiple skills, but few will have everything they need. Commonly needed skills in the team, or accessible through other organisations, include:\nYou’ll often be involved in multiple projects, so it’s useful to share expertise across projects.\nDefining input features in clinically relevant ways is particularly important for health data science. For example, ‘cancer’ might be an important input at an individual level; does this mean currently active cancer? Within the last five years? Any exclusions? Time needs to be allocated to work through and validate these definitions with clinicians.\nAdvisory groups are a good way to elicit feedback. Early in any project try to meet with a group of experts and discuss your research plan. Experts will always have different perspectives on the research and the context in which it will be used (see End-user engagement below, which also covers Māori engagement and co-design).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#team-capability",
    "href": "collaboration.html#team-capability",
    "title": "3  People, capability & collaboration",
    "section": "",
    "text": "subject matter expertise in data\nmachine learning\nsystem design\nuser interface design\nclinical\ngovernance\nconsumer/patient and specifically impacted communities\nlegal and privacy\nethics\nimplementation and change management.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s particularly important to allow plenty of time for clinical input to data understanding and preparation for data science.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#end-user-engagement",
    "href": "collaboration.html#end-user-engagement",
    "title": "3  People, capability & collaboration",
    "section": "3.2 End-user engagement",
    "text": "3.2 End-user engagement\nYou should engage end users (usually consumers and clinicians) early in co-design, to understand how outputs can be tangible for those who will use them or be impacted by them (see Transparency, interpretability, and explanation). It’s important to understand workflows and where tools and/or models may be used. End-user engagement will also help to drive IT implementation if the benefits are clearly articulated to the appropriate stakeholders so work can be prioritised.\n\n3.2.1 Impacted customers/patients/groups\nData scientists are unlikely to have the correct context or cultural awareness to fully grasp what the data is telling them. This includes Māori and other ethnicity groups, consumers, and perspectives that cover age and ability ranges.\n\n\n\n\n\n\nNote\n\n\n\nAny recommendations for changes or improvements should be interpreted through the lens of the groups they are likely to affect, acknowledging the principle of “nothing about us without us”.\n\n\nHQSC’s Code of expectations for health entities’ engagement with consumers and whānau is a useful framework to bear in mind.\n\n\n3.2.2 Clinicians\nCo-designing from the start and having a clinical champion/sponsor to ensure that developments can be incorporated in existing workflows so they can be used will set you up for success (see Operational deployment).\n\n\n\n\n\n\nTip\n\n\n\nClinical engagement often makes or breaks a project - both to ensure efficacy and to advocate and promote use of the model in practice. In general, clinicians will adopt tools and models that solve a genuine problem, fit into their workflow patterns, and save them time or enhance safety. It is worth thinking about the different professions that might be involved (medical, nursing and allied professions) as well as specific skills that some professions bring such as in epidemiology, public health and biostatistics. There are also other groups involved in delivery of care that are not professionals but may have useful insights (eg. data analysts, administrators, commissioners).\n\n\nInterpretability (which is defined as understanding the reasoning behind predictions and decisions made by the model) is also key. Important stakeholders are unlikely to have lots of time to learn about your work, so displaying it in an easy-to-understand way is essential. You shouldn’t assume that everyone will interpret outputs in the same way, and understand what action is then required. Keep in mind that prospective customers may not be technologically or mathematically savvy, too.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#collaboration",
    "href": "collaboration.html#collaboration",
    "title": "3  People, capability & collaboration",
    "section": "3.3 Collaboration",
    "text": "3.3 Collaboration\n\nAgree collaboration will be be supported across different organisations (who may all be using different software and systems and have limitations around what can be used).\nRole definition, responsibilities\nRegular check-ins/stand-ups are helpful\nBe mindful of individuals’ schedules and availability, particularly clinicians.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#measuring-success",
    "href": "collaboration.html#measuring-success",
    "title": "3  People, capability & collaboration",
    "section": "3.4 Measuring success",
    "text": "3.4 Measuring success\n\nBe clear what success looks like. This will be different for research vs. implementation.\n\nAre you looking to learn/measure something specific?\nAre you expecting secondary or downstream benefits\n\nMaking sure the bigger picture is kept in mind - not getting lost in the detail in a way that doesn’t add value\nHave you reached the point of diminishing returns for investing further in model development?\nIf you have measurable benefits as an objective, you should develop a benefits realisation plan.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#project-management",
    "href": "collaboration.html#project-management",
    "title": "3  People, capability & collaboration",
    "section": "3.5 Project management",
    "text": "3.5 Project management\n\n\n\n\n\n\nTip\n\n\n\nData science is an iterative process of development. You’ll progressively learn more about the data, relationships in the data and how effective modelling is.\n\n\nGood project management or product development practices should be applied to data science projects, while accepting that these projects are often experimental or exploratory in nature. Some important project management considerations include:\n\nData issues flow ‘downstream’ and can impact every other part of a project. Ensure that sufficient time is allocated upfront to review and correct data quality (this also often happens in cycles - the analysis reveals quirks in the data that can be explored further and corrected)\nProject management should strive for continuous visibility, demonstrable progress. How are we tracking against timelines, budget, and the desired outcome?\nCreate open feedback channels where possible. Think about how to develop mockups or prototypes as early as possible for feedback - can you start with a very simple model, Excel dashboard or static design to help ensure that what you are developing will deliver value?\nHealthcare data science projects will often span multiple organisations\nDocumenting failures and lessons learnt helps prevent repeating mistakes.\nUse technology for progress tracking. Some options include:\n\nJira,\na spreadsheet-based activity tracker,\nTrello board,\nemailed summaries of actions and next steps",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "collaboration.html#governance",
    "href": "collaboration.html#governance",
    "title": "3  People, capability & collaboration",
    "section": "3.6 Governance",
    "text": "3.6 Governance\nGood governance is fundamentally concerned with the value and risk of a project, and must be established to ensure accountability and oversight. Developing policies and procedures can help you manage risk and clearly articulate principles of accountability, transparency, ethical use, privacy, and consent.\nYou should define and tailor a governance approach based on the needs of your own organisation.\nUsually a data science project is undertaken within a wider programme of work, which has its own governance structure and processes. Occasionally, more substantial initiatives will require their own standalone governance.\nGovernance may be applied at different levels, such as governance of data (inputs) or governance of models (product/output). People involved in model governance should come from diverse demographic and technical backgrounds, including perspectives of consumers and/or those who are impacted by the outputs of your work. People involved in data governance need to have an understanding of data flows - how data is captured, stored and used.\nQuestions to ask:\n\nAre ethics applications required?\nWhat existing governance groups and/or processes would need to be involved?\nWhich policies need to be followed?\nWhat level of documentation is required throughout?\nWill this be shared publicly? If so, how?\nWho is responsible for signing off?\nIs a new governance structure or process required here?\nWhat maintenance and/or ongoing review may be required? The NICE Evidence Standards Framework for Digital Health Technologies has a helpful list of points to consider.\n\nThere are many resources for governance groups and executive teams who are looking to write data policies and procedures. These include:\n\nNew Zealand Data and Information Management Principles (Digital.govt.nz): The government’s open data policies and best-practice guidance for agencies managing how data is stored, published and used\nNational Ethical Standards for Health and Disability Research (NEAC): These standards set out the ethical requirements for researchers, health service providers and disability service providers and apply whether or not research or quality improvement activities require review by an ethics committee\nPrivacy Act 2020: the legal basis for NZ organisations to work with data safely - also see Ethics & privacy\nHealth Information Privacy Code 2020 - also see Ethics & privacy\nHISO 10064:2017 Health Information Governance Guidelines (Manatū Hauora Ministry of Health): Guidance to the health and disability sector on the safe sharing of health information\nNew Zealand Government Open Access and Licensing framework (NZGOAL) (Data.govt.nz): For those who work for a government agency and want to enable appropriate re-use of your agency’s material by licensing its copyright works or releasing non-copyright material (such as open data) for re-use\nA Path to Social License: Guidelines for Trusted Data Use (Data Futures Partnership): August 2017 summary with continuing relevance around what New Zealand people expect from guidelines for data use and sharing\nLayered model for AI governance(Harvard University): A conceptual framework for thinking about governance for AI.\nLessons learned from developing a COVID-19 algorithm governance framework in Aotearoa New Zealand: Practical considerations from a governance group\nAlgorithm Charter (Data.govt.nz): A set of principles which have been signed off by most NZ government organisations (Manatū Hauora Ministry of Health)\nHealth and Disability Ethics Committees: the health organisation responsible for reviewing and approving research using health data and/or health system users - also see Ethics & privacy\nTe Mana Rauranga: a network of experts who have collected principles for Māori data sovereignty - also see Data sovereignty\nManatū Hauora Ministry of Health - Emerging Health Technology Advice & Guidance\nThe Law Foundation report on government use of AI in NZ (Colin Gavaghan et al. 2019)\nTrustworthy AI in Aotearoa: AI Principles report from AI Forum (AI Forum 2020)\n\n\n\n\nManatū Hauora Ministry of Health. “The algorithm charter”. https://www.health.govt.nz/our-work/digital-health/digital-health-sector-architecture-standards-and-governance/algorithm-charter.\n\nColin Gavaghan, Alistair Knott, James MacLaurin, John Zerilli, and Joy Liddicoat. 2019. “Government use of artificial intelligence in new zealand”., Wellington. https://data.govt.nz/assets/data-ethics/algorithm/NZLF-report.pdf.\n\nAI Forum. 2020. “Trustworthy AI in aotearoa”. https://aiforum.org.nz/wp-content/uploads/2020/03/Trustworthy-AI-in-Aotearoa-March-2020.pdf.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>People, capability & collaboration</span>"
    ]
  },
  {
    "objectID": "data-landscape.html",
    "href": "data-landscape.html",
    "title": "4  The unique health data landscape",
    "section": "",
    "text": "4.1 Finding data\nData for health data science projects is everywhere! There’s no shortage of available data in New Zealand. However, keep the following concepts in mind when you are considering how to find data, or access data you may already be interested in:\nTo understand population trends and context, aggregated data sets and web tools are publicly available via the Ministry of Health and Statistics New Zealand (Stats NZ). Micro data (at the level of the individual) is available to researchers on application to National Collections or Stats NZ for Confidentialised Unit Record Files (CURFs). Dissemination of micro data is in accordance with the Privacy Act, health legislation and contracts and access is strictly controlled according to use.\nUseful resources:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The unique health data landscape</span>"
    ]
  },
  {
    "objectID": "data-landscape.html#finding-data",
    "href": "data-landscape.html#finding-data",
    "title": "4  The unique health data landscape",
    "section": "",
    "text": "Data can only be used for the purpose for which it was collected; any other use is called “secondary purpose” and requires additional consent (see Use and re-use of data section)\nHealth data used for research purposes needs thought around how the research will be conducted efficiently, ethically, and with privacy and safety front-of-mind.\n\n\n\n\nThrough a review of Aotearoa New Zealand health datasets, PDH has produced an interactive and updatable list of data available in New Zealand. This includes data from Figure.nz and many other sources. (Precision Driven Health 2022)\nA recent research project which undertook a local algorithm scan produced a whitepaper report on The future of healthcare algorithms in Aotearoa New Zealand (New zealand health sector algorithm scan 2021).\nWhere can I find health information? (Manatū Hauora): Data sources commonly used when analysing the health of New Zealand populations\nHealth statistics and data sets (Manatū Hauora)\n\n\nPrecision Driven Health. 2022. “Aotearoa NZ data sources review”. https://data.precisiondrivenhealth.com.\n\nNew zealand health sector algorithm scan. 2021. https://precisiondrivenhealth.com/new-zealand-health-sector-algorithm-scan/.\n\n4.1.1 The Integrated Data Infrastructure\nSupported by Statistics NZ, the Integrated Data Infrastructure (IDI) is a repository of individual-level data from multiple government sources, able to be linked together, anonymised, and used for research. It’s a massive resource which is accessible through a prescribed process with very high safety and privacy requirements. It may not be possible to derive individual-level insights from IDI data.\nThe Virtual Health Information Network includes many researchers who are using the IDI. VHIN’s IDI guides can help you understand what centralised data is available and how it could be used in your project.\nIf you’re interested in what you could do with the IDI, it’s best to make contact with a researcher who already has experience in using this platform. Refer to the list of research using Stats NZ microdata.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The unique health data landscape</span>"
    ]
  },
  {
    "objectID": "data-landscape.html#understand-the-data",
    "href": "data-landscape.html#understand-the-data",
    "title": "4  The unique health data landscape",
    "section": "4.2 Understand the data",
    "text": "4.2 Understand the data\nKnowing all the data you’re able to access is critical to understanding what is possible, in addition to understanding the question or problem you are trying to solve.\n\n\n\n\n\n\nTip\n\n\n\nHealthcare data is often complex and ‘dirty’ (inaccurate, incomplete or inconsistent). When possible, liaise with analysts who work within the organisation to gain a local understanding of the data.\n\n\nHealth data is often only available after a significant lag time, and with a slow refresh rate. For example, there is a specific chain of events that lead to updates to national health data collections, and this can take a significant amount of time - often many months.\nAt an early stage, it is valuable to consider:\n\nData landscape - What data is collected? How is access managed? Does it help address the problem you are trying to solve?\nCharacteristics - What is the format, type and size of data? When was the data collected? How often will you receive it?\nAvailability and quality - How much historical data is available, and what is the quality? Use the minimum necessary!\nPurpose - What purpose was the data collected for, and does that influence your interpretation?\nConsent - Is the use of data covered by existing patient consent?\nData collection, maintenance, publication - Distinguish between data already collected and new data created by the study. How do you plan to maintain and/or publish this data?\nPersonally identifiable information - Is de-identification required? Who will do this? (See Data identifability)\nData availability - How long will it take to source and are there any reporting or system lags?\nLabelling/annotation - Does the project need Human In the Loop (HITL) mechanism for annotating and validating the input and output data?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The unique health data landscape</span>"
    ]
  },
  {
    "objectID": "data-landscape.html#ethics-privacy",
    "href": "data-landscape.html#ethics-privacy",
    "title": "4  The unique health data landscape",
    "section": "4.3 Ethics & privacy",
    "text": "4.3 Ethics & privacy\nConcepts around the legal, privacy, and ethical dimensions of health data projects are often interlinked. Legal perspectives consider the fit of the project with the laws of the data source jurisdiction as well as any legal requirements of the analysis location, if these aren’t the same place. Privacy perspectives are around what data is collected, how it’s collected safely, where it is stored, and its lifecycle. Ethical conduct of a project includes ensuring the risks to participants of data use are outweighed by the benefits of the project.\nAll research in New Zealand which uses data from humans needs to be undertaken in an ethical manner. In some cases, approval from a registered ethics committee is required before the project can be started. In many cases, the organisation undertaking the research (such as your employer) may also have specific research or ethical approval processes as well.\nWhen dealing with health data, be careful when assessing if a data science project requires ethical approval. Ethical approval is typically required for any evidence-generating studies, or studies which propose changes to current standard of care (before any model is evaluated/validated) and should always be sought prior to accessing patient data. Ethical approval is given through a written application process and gives permission for the research to be conducted by named investigators during a specific time period.\nEthics approvals in New Zealand are provided through committees which have had centralised approval to follow appropriate standards. The ethical standards are set by the National Ethical Standards for Health and Disability Research and Quality Improvement and apply to researchers, health service providers and disability service providers, regardless of whether or not an additional approval process is required. Think of these like a set of best practices for undertaking work with health data or human participants - follow them regardless of whether anyone is requiring you to do so!\nMost formal approvals for health data research are provided at a national level by the Health and Disability Ethics Committees (HDEC); in some cases a more localised approval is required instead or in addition (for example, the Auckland Health Research Ethics Committee).\nOn top of this, hospitals or clinical organisations usually have their own research offices which require separate notification (such as Research Office, Hauora a Toi Bay of Plenty, Te Whatu Ora).\nFind out if your study requires HDEC review on the HDEC website.\nMany of the questions asked in an ethics application and approval process are important in evaluating the risks and benefits and can indicate if there is social licence for the intended research. This process forces researchers and organisations to consider whether their work has a net benefit for society. The concept of “social license” or societal acceptance for use of health data is a related idea (see Social license - use of health data).\n\n\n\n\n\n\nImportant\n\n\n\nSeek appropriate ethics approvals prior to accessing patient data.\nThe implications of assuming that ethics is not required can have significant downstream effects on a project, such as reputation issues or delays. HDEC can provide ‘Scope of Review’ services to advise if the research you are doing is considered exempt from requiring an ethics application. Consider seeking written evidence of this as an assurance for stakeholders. See the HDEC website for the current process.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSome ethics applications can take months before an approval is granted, so allow adequate time for this process, factoring in when review meetings are held. It’s also likely that further questions may be asked at this review stage.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nConsider if existing patient consent is sufficient to cover use of the data. See Consent below.\n\n\nGenerally all use of administrative health data for research purposes will need to go through the HDEC process. Consent for use of administrative data is mainly around its use for improving the care of that individual within the health service or quality improvement processes (defined as audits or other activities). Unless the work is specifically with business intelligence within a healthcare organisation, you should ask for assurance to confirm if ethical approval from HDEC is required, or not.\nIn New Zealand, health information follows the Health Information Privacy Code 2020. The rules of this code can be summarised for health agencies as below. These principles are also highly relevant to the use of health data for research purposes.\n\nOnly collect health information if you really need it.\nGet it straight from the people concerned where possible.\nTell them what you’re going to do with it.\nBe considerate when you’re getting it.\nTake care of it once you’ve got it.\nPeople can see their health information if they want to.\nThey can correct it if it’s wrong.\nMake sure health information is correct before you use it.\nGet rid of it when you’re done with it.\nUse it for the purpose you got it.\nOnly disclose it if you have a good reason.\nMake sure that health information sent overseas is adequately protected.\nOnly assign unique identifiers where permitted.\n\nPrivacy Impact Assessments (PIAs) should also be conducted at an early stage to identify potential data protection risks on the data of the individuals included. Measures should be adopted to eliminate or mitigate risks. The Office of the Privacy Commissioner offers some guidance on PIAs.\nUseful resources:\n\nA checklist to address ethical and governance questions\nA Research Ethics Framework for the Clinical Translation of Healthcare Machine Learning\n\n\n4.3.1 Consent\nYou need consent before you can use data - particularly data that contains sensitive information. In the General Data Protection Regulation (GDPR), a regulation in EU law on data protection and privacy, consent is defined as:\n\n“any freely given, specific, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.”\n\nSome health providers give patients the opportunity to opt in for their data to be used for research and operational purposes, or to improve their care. Opting in may or may not explicitly allow for the processing of their de-identified data by third parties. More commonly, the patient hasn’t provided explicit consent.\nWhether it’s appropriate to use this data will depend on context (for example public good vs. commercial gain), the degree to which the data is anonymised, whether data is provided in aggregated form, and the purpose the data is being used for.\n\n\n4.3.2 Use and re-use of data\n‘Use’ of data relates to using data for the purpose for which it was consented and collected. Re-use of data (or secondary use) is when you use data collected for another purpose.\nWhen you re-use data, you should be mindful of its purpose, coverage, bias, timeliness, and applicability to the secondary use. For instance, the National Minimum Data Set is gathered for policy formation, performance monitoring, research and review. It may be useful for understanding hospitalisations, but does not provide a complete picture of an individual’s health journey.\n\n\n4.3.3 Social license - use of health data\nSocial licence is the implied permission to make decisions about the management and use of the public data in a way that ensures trust and confidence in the way that data is managed. Organisations who store health data are stewards.\n\n\n\n\n\n\nImportant\n\n\n\nAny use of data, outside of that which has been explicitly consented, is subject to ethics and requires consideration of social licence.\n\n\nConceptually, the level of engagement required to gain social license is higher for decisions that are more likely to affect an individual and are fully automated, compared to those that are manual and affect an entire population. Social licence isn’t ‘gained’ or ‘approved’; organisations need to gauge people’s thoughts, feelings, perceptions on the use of their data. These attitudes are dynamic and constantly evolving.\nAssurance to stakeholders should be provided through transparency and governance to limit reputational harm, particularly when using patient data in a commercial context. Consider what data you’re handling, how identifiable it is and if patient consent is specifically required. This is particularly important for data that may be re-used for secondary purposes - the original consent should be carefully reviewed.\nData should be used for public good in a way that is equitable, with consideration for any unintended consequences, such as increased clinician workload (due to workflow disruption), potential for misuse of the model, and perpetuating biases that exist in the data. Data needs to be treated with care and a suitable data management plan can help.\nIn our experience, privacy, confidentiality, security, transparency, communication and the purpose for which data is used, are often raised as concerns by clinicians, administrators, legal experts, and other stakeholders.\nUseful resources:\n\nA Path to Social Licence Guidelines for Trusted Data Use - Data Futures Partnership 2017 (PDF)\nManatū Hauora research on social licence for health data re-use",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The unique health data landscape</span>"
    ]
  },
  {
    "objectID": "data-landscape.html#sovereignty",
    "href": "data-landscape.html#sovereignty",
    "title": "4  The unique health data landscape",
    "section": "4.4 Data sovereignty",
    "text": "4.4 Data sovereignty\nData sovereignty refers to the understanding that data is subject to the laws of the nation within which it is collected and stored. In New Zealand, there is also a focus on where the data is stored and processed. Data agreements should take care to address these points so they are clear to all parties. Data handling and ethical dimensions of the project also need to ensure data sovereignty is at front of mind.\nMāori data sovereignty recognises that Māori data should be subject to Māori governance. Māori data sovereignty supports tribal sovereignty and the realisation of Māori and iwi aspirations. Māori must be included in any work about Māori data. An equity lens is required and ideally should include Māori in the research team as well as in external review/advisor roles.\nUseful resources:\n\nTe Mana Raraunga (Māori Data Sovereignty Network) has helpful resources and guidance.\nThe Health Research Council of NZ (HRC) also produces Guidelines for Researchers on Health Research Involving Māori.\nNZ government’s Principles for the safe and effective use of data and analytics (Government Chief Data Steward and Office of the Privacy Commissioner 2018)\n\n\n\n\nGovernment Chief Data Steward and Office of the Privacy Commissioner. 2018. “Principles for the safe and effective use of data and analytics”. https://www.stats.govt.nz/assets/Uploads/Data-leadership-fact-sheets/Principles-safe-and-effective-data-and-analytics-May-2018.pdf.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The unique health data landscape</span>"
    ]
  },
  {
    "objectID": "data-access.html",
    "href": "data-access.html",
    "title": "5  Accessing and managing data",
    "section": "",
    "text": "5.1 Planning data access\nTo understand data access requirements we must consider the sensitive nature of health data, consent, security and technical needs.\nKnow what you are asking for. Your data request should be well specified and considered and it will take some time, discussion with the health data provider and analysis to formulate.\nMany providers will have strict data transfer and storage protocols, however these may not always be followed by individuals. How the data will be transferred and stored, and at what frequency should be discussed and agreed, before a .csv file suddenly lands in your inbox.\nData that contains Protected Health Information (PHI) or Personal Identifiable Information (PII) cannot be freely shared, however after de-identification (see Data Identifiability) it may be possible to share such data, under certain terms.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-access.html#data-sharing-agreements",
    "href": "data-access.html#data-sharing-agreements",
    "title": "5  Accessing and managing data",
    "section": "5.2 Data sharing agreements",
    "text": "5.2 Data sharing agreements\n\n\n\n\n\n\nTip\n\n\n\nData sharing agreements should be written and agreed early, ideally guided by legal advice.\n\n\nData sharing agreements between a data provider and data recipient typically cover:\n\nWhat data will be shared and for how long\nWhat is the consented use of the data\nHow the data will be shared and stored\nHow and when the data will be destroyed after project completion\nPrivacy and confidentiality\nHow security will be ensured\nHow compliance with and breaches of the agreement will be handled\nWhich parties are responsible for each activity, such as de-identification of data.\n\nThese agreements cannot escape data protection and privacy laws, and agreements that already exist between a data provider and a patient. This should be taken into consideration before any data is transferred.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-access.html#the-data-request",
    "href": "data-access.html#the-data-request",
    "title": "5  Accessing and managing data",
    "section": "5.3 The data request",
    "text": "5.3 The data request\nAnalysis and discussion regarding the data request are also vital at an early stage. Health data is sensitive and describes people in their most personal and vulnerable moments. It is a privilege to be working with it, so be specific with your request and don’t seek more than is really needed.\nSome questions to consider are:\n\nWhat data sources are available and accessible?\nWhat fields are needed?\nCan the data be operationalised?\nWhat time period is to be covered?\nHow do privacy, ethics and equity considerations affect my data request?\nWhat is the minimum data set that I will need for this project?\nWho has the ability to provide the data?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-access.html#waiting.and-more-waiting",
    "href": "data-access.html#waiting.and-more-waiting",
    "title": "5  Accessing and managing data",
    "section": "5.4 Waiting….and more waiting",
    "text": "5.4 Waiting….and more waiting\nOnce you have your plan in place, smooth sailing isn’t guaranteed. De-identification, data sharing agreements and data requests can take time. But even when an agreement is signed and plans are ready, you might find a lot more time passes while waiting for data.\nHealth data providers usually have to undertake a lot of work before sending data in the form agreed. Data may be located in different systems, extraction may require booking in technical resources and the data may need pre-processing and annotation. Also, while you may be eagerly awaiting the data, it may not be afforded the same priority by a busy health provider. Patience is a virtue, but the occasional gentle nudge may be necessary. You also need to manage your own workload, and you don’t want all of your data buses arriving at once.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-access.html#transferring-data",
    "href": "data-access.html#transferring-data",
    "title": "5  Accessing and managing data",
    "section": "5.5 Transferring data",
    "text": "5.5 Transferring data\nSome providers may require that you use their systems, whether on premises or in the cloud, so that the data never ‘leaves’ their organisation. Others may be happy to upload their data to a secure cloud-based server provided by you, and some may have their own file transfer systems.\nTry to avoid email for data transfer, even if a file is password protected. Emailed data can be difficult to track and audit. This may result in multiple copies being saved using up storage capacity. Files are also frequently too large to be attached. Security is a particular concern, especially where either sender or recipient uses a third-party email service – you do not know how many servers are handling that message.\nIf you will be receiving data regularly, it’s also better to minimise the number of transfers by having data sent in batches. Any data you receive should also be different from what you have already received. Have this conversation with the health data provider at the planning stage.\nFinally, please ensure you confirm successful receipt of the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-access.html#data-governance-and-data-management",
    "href": "data-access.html#data-governance-and-data-management",
    "title": "5  Accessing and managing data",
    "section": "5.6 Data governance and Data management",
    "text": "5.6 Data governance and Data management\nData governance and data management are distinct concepts with some areas of overlap and linkage. Both areas concern how we access, store, log, secure, maintain, use, share and destroy data in an efficient and standardised way, guided by principles such as accountability, transparency, ethical use, privacy, consent and data availability, integrity and quality.\n\n\n\n\n\n\n\nData governance\nData management\n\n\n\n\nMaintains oversight, provides accountability, shapes and communicates policies and procedures\nEnacts policies and procedures for day-to-day handling of data\n\n\nGovernance group or executive team or board; a diverse group of stakeholders to cover multiple perspectives\nIndividual data stewards\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis section is written from the perspective of a consulting practice, research organisation or any organisation that uses data which it does not directly collect.\n\n\n\n5.6.1 Data Governance\nData governance is a specific sub-discipline (West et al. 2020).\n\nWest, K, Hudson, M, and Kukutai, T. 2020. “Data ethics and data governance from a māori world view”. in Advances in research ethics and integrity. eds. L. George, J. Tauri, and L.T.A.o.T. MacDonald., 67–81. https://www.emerald.com/insight/content/doi/10.1108/S2398-601820200000006005/full/html.\nSee the Governance section.\n\n\n5.6.2 Data Management\n\n5.6.2.1 Storage and security\nWith a data access plan in place, and an understanding of what is available, appropriate storage and security requirements need to be in place.\nWhen storing data on premises or on your secure cloud-based service, there should be a clear process for access control. Larger, regular batches of data are preferable, but where there is irregularity in timing and peaks in volume, cloud based auto-scaled storage may help keep costs down.\nOn-premises data storage should be on a secure server, and not stored or replicated on personal computers.\n\n\n5.6.2.2 Data register or log\nWhat starts out as a data trickle can quickly become a data flood. It is worthwhile to maintain a register of data from the first file received.\nA data register or log can help you or your organisation track:\n\nwhat data you have received\nwhen you received it\nwhere you are storing it\nwho can/should have access to it\nwhen it should be destroyed\nwhat processed data sets exist and where, particularly if they contain PHI\n\nThe format can be tailored to suit your organisation (for example, using a spreadsheet).\n\n\n5.6.2.3 Tools for data management\nManaging data for a small organisation or research team could be as simple as having a secure storage solution and a spreadsheet to keep track of important metadata. Larger organisations will likely need formal data governance, data policies in place and data stewards to manage the use, storage and security of data. Tailor your approach based on the data being handled.\nConsider version control for all aspects of the project, including data; depending on the tooling that you use, this may already be an included capability. DVC is one tool designed specifically for data version control for machine learning. Your development environment or cloud storage solution may also include features of this type. Data files can be named with versions, however this method is not as robust as automated versioning that is managed by a tool.\nData management tools are responsible for carrying a wide range of data or documents.\n\n\n5.6.2.4 Useful references\n\nThe Data Management Body of Knowledge (DM-BOK) (Earley et al. 2017)\n\n\n\n\nEarley, S, Henderson, D, and Association, D M (eds.). 2017. DAMA-DMBOK: Data management body of knowledge. 2nd edition Basking Ridge, New Jersey.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Accessing and managing data</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html",
    "href": "data-identifiability.html",
    "title": "6  Data identifiability",
    "section": "",
    "text": "6.1 What information is sensitive? (PII and PHI)\nDifferent jurisdictions define sensitive data differently. The US federal law Health Insurance Portability and Accountability Act of 1996 (HIPAA) required the creation of national standards to protect sensitive patient health information from being disclosed without the patient’s consent or knowledge. The definitions provided by HIPAA are important to consider even when you are not working in the United States or with data from the USA.\nThe HIPAA defines PII and PHI as:\nNew Zealand’s National Ethical Standards provide a shorter list of direct and indirect identifiers:\nYour organisation may have a different name or definition for this information. However it is named or defined, it’s important to note that health data contains sensitive information that patients would not want disclosed. This data is a privilege to use and should be treated with utmost care.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#what-information-is-sensitive-pii-and-phi",
    "href": "data-identifiability.html#what-information-is-sensitive-pii-and-phi",
    "title": "6  Data identifiability",
    "section": "",
    "text": "PII - Personally identifiable information\nPHI - Protected Health Information\n\n\n\n\nAny piece of information that can be traced to an individual’s identity, not necessarily health related (e.g. address).\n​​Any piece of information in an individual’s medical record that was created, used, or disclosed during the course of diagnosis or treatment that can be used to personally identify them. HIPAA has a detailed definition of PHI.\n\n\n\n\n\n\n\n\n\nLook out for these fields, which HIPAA considers as identifiers\n\n\n\n\n​​Name\nAddress (including subdivisions smaller than state such as street address, city, county, or zip code)\nAny dates (except years) that are directly related to an individual, including birthday, date of admission or discharge, date of death, or the exact age of individuals older than 89\nTelephone number\nFax number\nEmail address\nSocial Security number (in the USA)\nMedical record number or NZ National Health Index (NHI) number\nHealth plan beneficiary number\nAccount number\nCertificate/license number\nVehicle identifiers, serial numbers, or license plate numbers\nDevice identifiers or serial numbers\nWeb URLs\nIP address\nBiometric identifiers such as fingerprints or voice prints\nFull-face photos\nAny other unique identifying numbers, characteristics, or codes\n\n\n\n\n\nIdentifier types from the HISO 10064:2017 Health Information Governance Guidelines via the National Ethical Standards\n\n\nDirect identifiers\nIndirect identifiers\n\n\n\n\nNHI\nDate of birth\n\n\nName\nIdentification of relatives\n\n\nStreet address\nIdentification of employers\n\n\nPhone number\nClinical notes\n\n\nOnline identity (e.g., email, Twitter name)\nAny other direct or indirect identifiers that carry significant risk of re-identification\n\n\nIdentification numbers (e.g., community services card, driver’s licence)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#how-do-i-deal-with-sensitive-data",
    "href": "data-identifiability.html#how-do-i-deal-with-sensitive-data",
    "title": "6  Data identifiability",
    "section": "6.2 How do I deal with sensitive data?",
    "text": "6.2 How do I deal with sensitive data?\nFrom the regulatory standpoint: the HIPAA has a “Privacy Rule” which requires safeguards when working with protected health information (PHI). Two methods of handling sensitive information which satisfy the HIPAA Privacy Rule are Expert Determination and Safe Harbor.\nWhen a data provider transfers data to you, first check whether you should be in receipt of that data. This requires you to have processes to check the data for PII and PHI. You should also have procedures in place for what to do should you find any PII or PHI, including the destruction of the data, and reporting of the incident.\n\n\n\n\n\n\nImportant\n\n\n\nIf a health data provider has provided you access to PHI or PII containing data in contravention of a data sharing agreement, continued access to or retention of that data is not defensible by reasoning that the mistake was theirs. A guiding principle is that appropriate storage, transfer and use of data is the responsibility of all parties involved.\n\n\nCreating synthetic data is another method for research (see Use of Synthetic Data).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "href": "data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "title": "6  Data identifiability",
    "section": "6.3 Checking your data for the presence of identifiable information",
    "text": "6.3 Checking your data for the presence of identifiable information\nIn general, you are most likely to be working with de-identified data. The data provider is responsible for ensuring that data released is compliant and the data receiver also has a responsibility for highlighting if this has not been done.\nIt is imperative to check that any dataset you receive meets the de-identification standard you are expecting; e.g. you have not been sent a dataset that contains identifiers when it should not.\n\n\n\n\n\n\nTip\n\n\n\nAs a minimum, do a common sense check of metadata and manually scan the first 1000 rows of a dataset for any PHI e.g. unique identifiers, address, and name.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#de-identifying-structured-data",
    "href": "data-identifiability.html#de-identifying-structured-data",
    "title": "6  Data identifiability",
    "section": "6.4 De-identifying structured data",
    "text": "6.4 De-identifying structured data\nDe-identification can be achieved through the suppression or transformation of certain identifying attributes. For example, a health data provider can be asked to exclude name fields, provide age range rather than date of birth, transform addresses to a statistical area unit, and/or encrypt unique identifiers prior to data transfer.\nEven with suppression or transformation of individual identifiers, individuals can still be identified in structured data where information about an individual is already known. For example, you know someone who is 57, has had breast cancer and lives in a certain postcode. If only one person in a dataset has these attributes, this information could be used to identify that person in the de-identified data. Particularly when linked to other data sources, more personal information could then be gleaned for that person.\nk-anonymisation or ε-differential privacy techniques can be applied to ensure that individuals can’t be identified via combinations of attributes. Both techniques involve a trade-off of privacy and data utility.\n\nk-anonymisation is where at least ‘k’ individuals share an identifying set of attributes for any individual. It can be achieved through suppression of attributes or the generalisation of values (for instance, using age ranges).\nε-differential privacy involves adding noise to the original distribution in a way that ensures that the probability that a statistical query will produce a given result is nearly the same on a dataset that has had one person’s information removed. The higher the level of de-identification, the more noise is added to the distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#de-identifying-unstructured-data",
    "href": "data-identifiability.html#de-identifying-unstructured-data",
    "title": "6  Data identifiability",
    "section": "6.5 De-identifying unstructured data",
    "text": "6.5 De-identifying unstructured data\nUnstructured data (which is data that doesn’t have a predefined format) poses a particular challenge for anonymisation. It’s estimated that approximately 80% of health data is stored as unstructured text, which is a form of unstructured data.\nBasic pattern matching using regular expressions may go some way to locating identifying text and can be useful for finding attributes with known formats (for example NHI or date of birth). However, free text, even in very short phrases, can contain a sometimes surprising amount of identifying information. Natural language processing techniques and machine learning can be applied for more sophisticated textual de-identification, with the support of de-identification tools.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-identifiability.html#tools",
    "href": "data-identifiability.html#tools",
    "title": "6  Data identifiability",
    "section": "6.6 Tools",
    "text": "6.6 Tools\nSoftware tools which can be of help with the processes of identifying sensitive data and removing it include:\n\nDe-identifier (Orion Health): can identify and remove PHI and PII from datasets and databases to a customisable level of de-identifiability/reidentification risk\nMacie (Amazon): able to check data stored in Amazon Web Services (AWS)\nComprehend Medical (Amazon): provides a PHI detection API\nDLP (Google): able to check data stored in Google Cloud",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data identifiability</span>"
    ]
  },
  {
    "objectID": "data-preparation.html",
    "href": "data-preparation.html",
    "title": "7  Data preparation",
    "section": "",
    "text": "7.1 Data wrangling\nData wrangling is the process of cleaning and unifying complex datasets for easy access and analysis. Good data wrangling practice is based on comprehensive knowledge of the data. We recommend the following:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data-preparation.html#data-wrangling",
    "href": "data-preparation.html#data-wrangling",
    "title": "7  Data preparation",
    "section": "",
    "text": "For a given data source, learn as much as you can about its collection, storage, how it’s updated and maintained, the definition and dependency of each data item, and the limitations. There are more upfront ethical considerations when dealing with health data including consent, ethics approval and understanding the equity and clinical context.\nFor local data collection, consider how to store it (in files, in a database, locally or on cloud etc.) to make information retrieval easier, or/and data sharing easier (see Data Management).\nConsider the size of the data - should it be processed all at once, batch processed or stream processed? Stream processing is the processing of records as a stream of data, for example record by record. Your computing location (on your computer, on a local server, or on a cloud provider) may affect your decision here.\nConsider if the data needs de-identification (see Data identifiability).\nMaintain an equity lens for any type of data science work, including building and evaluating models (see Bias in data).\nInvolve clinical leads when data wrangling. Within healthcare, the risk associated with some data (for example, laboratory data or vitals) is often distributed at the extreme ends. It’s difficult for data scientists to know if the risk is distributed linearly without the clinical context.\n\n\n7.1.1 Data labelling\nData labelling can have a significant impact on the success of a data science project. A label is a category you might tag a record with for a model to learn. For instance, you might label an ED admission according to whether that patient is high priority for triaging, or you might label a medical alert as containing an adverse drug reaction. When learning, a model finds patterns in the data attributes that map to the labels. In the triage example, attributes that are mapped to the label “is high priority” might include body temperature, blood pressure, heart rate and oximeter reading.\nAim for labels that are clearly and appropriately defined, involving multiple qualified labellers.\n\n7.1.1.1 Defining labels\nBe precise, and aim to align the definitions with potential use cases. For example, radiology reports might already be labelled according to the nature of the report findings, such as there was a “finding”, or there was a “finding that is not of concern” or there was “no finding” etc. However, if our goal is to know whether a report requires follow-up and we would like to use machine learning techniques over those reports and labels to achieve this, the initial labels may not be useful: some reports that have a “finding that is not of concern” may need follow-up and some may not. The labels “follow-up required”, or “no follow-up required” are more directly aligned to the use case here.\nBe clear. A clear label definition will help both human labellers and model performance. If the boundary between labels is fuzzy, a person will either struggle to choose a label for a data record, or will apply labels inconsistently. If labels are inconsistent in the training set, then we might reasonably expect that the model will also struggle to find the boundary between labels. In the radiology report example, imagine that the use case aligns with categorising reports according to findings. Consider one report showing that a person has a pre-existing mass considered benign, who then has a follow up scan that reveals the mass has not changed. Different human labellers would need clear definitions to be able to consistently label this report. Without a clear definition, one human labeller may decide that there is a “finding that is not of concern” based on the presence of a mass of no concern, while another may decide that there is “no finding” given nothing has changed since the last scan.\nWhen there is a period of time between labelling iterations, the same human labeller may make inconsistent labelling choices. Clear label definitions will also help with label consistency over time.\n\n\n7.1.1.2 Many labellers make models work\nEven with clear definitions, it is normal to find that different human labellers may vary in how they apply a labelling strategy with no one person being the “source of truth”. Inter-labeller variation should be expected. To reduce the effects of this variation, involve many labellers so that each record has labels assigned by two or more people. Labellers will disagree, so when training a model, you could either decide to use records with labels that human labellers unanimously agree on, or those that the majority of labellers agree on, excluding the records with label disagreement from the training set.\nThe reality is that data labelling in a health context often requires highly skilled and experienced practitioners working under time constraints. If you only have one person who can provide the time required (for what can be a tedious task), then investment in clearly defining labels upfront is especially important.\nFor ongoing labelling efforts, including labelling in a clinical workflow may be an efficient strategy.\n\n\n\n7.1.2 Data quality\nIt’s important to conduct a data quality review as an early step and produce statistical summaries for checking purposes. This process usually includes profiling the fields within a data set, visualising distributions, checking relationships between fields and exploring data completeness (including completeness over time). Involve the data owners and subject matter experts in this process and consider additional rounds of quality reviews if the data is complex.\nData profiling (for example, with Pandas-profiling) is recommended for both data quality checking and data understanding. Data profiling involves providing descriptive summaries and visualisations of the data for review with clinicians and subject matter experts.\nBasic aspects for data quality check include:\n\nData availability across time\nData distribution change across time\nOutliers: Do we need to cap variables with extreme values?\nData completeness/missingness\nDuplication in the data\nTarget label distribution (data imbalance)\nAssociation between features and the target\nData timeliness: The data resource provides the data in appropriate time.\nScalability: The data can be accessed from many components without losing its meaning\nProvenance: The data should be based on valid authority\nLocality: The location of data resource should be provided to check relevance of data\nStructure: form of data\nAdoption: The data are useful for the project purpose and adaptable with the requirements, including operational requirements (will the data be refreshed, timely and in the expected format?)\nIdentification of extreme/outlier values, and variables that require capping.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data-preparation.html#data-linkage",
    "href": "data-preparation.html#data-linkage",
    "title": "7  Data preparation",
    "section": "7.2 Data linkage",
    "text": "7.2 Data linkage\nMany projects that involve health data will require linking datasets on certain attributes (keys). For example we might link datasets by matching records that share the same NHI, by matching records that share a combination of personal details, or by matching locations.\nOne example is linking data across government departments or agencies, for instance linking hospitalisation data to vaccination data, matching on NHI. Another example is where individual health data is linked to census data based on census meshblock.\nData linkage refers to this joining of datasets. It allows us to get a more detailed picture of a person or entity. For instance, when linking medical records to census records, this might reveal that a patient resides in an area of high deprivation.\n\n7.2.1 Unique identifiers\nNHI is a unique health identifier that is commonly used to link datasets in health. As a unique identifier, each NHI should identify a single person and each person should have a single NHI. However, in certain cases multiple NHIs get recorded for a single person. We note that records with a single unique identifier, and those with multiple “unique” identifiers, can have equity considerations in health and indicate the nature of interaction with the health system.\n\n\n7.2.2 How to link\nIdeally datasets would be linked prior to de-identification, so that the linkage can be as robust as possible. Further, de-identification after linking can help minimise the risk of re-identification via linked data (see De-identifying structured data).\nWhere datasets need to be de-identified prior to linking at the level of the individual or patient, you will need to consider how to maintain the integrity of the linkage. Unique identifiers, such as NHI, may need to be encrypted, rather than suppressed, in the process of de-identification.\nWhere data comes from different sources, unique identifiers will need to be encrypted in the same way in order to link datasets. If the processed data is to be delivered back to the health provider at the level of the individual, then there needs to be the possibility of decryption of the identifiers or another way of linking the processed data back.\n\n\n7.2.3 Risk of re-identification\nTwo datasets that separately will not identify an individual may identify an individual once linked. As a result, the risk of re-identification of an individual should be considered prior to linking datasets. Ideally, datasets are linked together and de-identified prior to delivery from the health provider; however, this is often not possible.\n\n\n7.2.4 Approaches\nWhere data cannot be linked on a unique identifier, there are approaches that can be used to match records. The ‘deterministic’ approach is to match on a combination of attributes that will uniquely identify a person. An issue with this approach is that in a dataset that is de-identified well, it should be difficult to uniquely identify individuals based on a combination of unique attributes.\nThe alternative ‘probabilistic’ approach involves calculating conditional probabilities to determine the likelihood that a given pair of records match.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data-preparation.html#missing-data",
    "href": "data-preparation.html#missing-data",
    "title": "7  Data preparation",
    "section": "7.3 Missing data",
    "text": "7.3 Missing data\nMissing data without context should be interpreted carefully. For example, missing hospitalisation records might indicate different access to care or it could mean the data was not recorded; you may not have access, or you’re healthy and haven’t needed to see your GP. Care must be taken in interpreting missing data, as it can be hard to be sure of the context around the missing data. This illustrates why clinical engagement, and stepping back to see the wider picture, is vital in health data science projects.\nMissing data is a common problem with most health data sets. When you encounter missing data, you can choose how to manage it, by either:\n\nRemoving it\nInterpreting it as “not applicable”\nImputing it (fill it in with other values).\n\nOften a combination of removing excessively missing (according to a pre-determined threshold) observations and variables, and then imputing the remaining missing values, is effective.\nWhich option you choose should be based on whether your method is tolerant of missing values; whether the data appears to be ‘missing completely at random’ (MCAR) in which case it could be ignored; or whether there are some patterns in its missingness.\nIf you choose to remove the data, you’ll need to decide whether to remove variables or observations (for example, rows or columns).\nTry adding ‘proxy’ variables or altering data collection processes to avoid missing data as much as possible.\nMost data is unlikely to be MCAR. Often, data is more likely to be missing for particular reasons (defined as ‘missing not at random’ (MNAR)). Consider if there’s a reason for the missingness, as this can itself be informative. For example, data may be missing from smaller subgroups within the dataset. This is particularly important when considering stratification for ethnicity.\nIt’s important to consider the potential bias introduced into a model by removing missing data, or the impact on equity if data is removed. Before continuing with imputation or removal, check for patterns in the data of individuals who have partial missing data to assess the equity and bias implications of removing or imputing it. Look for evidence of non-random missingness by comparing the complete and missing data groups through stratification for important demographic variables such as age and ethnicity.\nImputation can be a useful technique for overcoming missing data problems, but can be computationally intensive. Through a PDH research project, a guide on multiple imputation was published with information about the application of imputation techniques. Your strategy for handling missing values will have implications for how you handle future unseen data too.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data-preparation.html#use-of-synthetic-data",
    "href": "data-preparation.html#use-of-synthetic-data",
    "title": "7  Data preparation",
    "section": "7.4 Use of synthetic data",
    "text": "7.4 Use of synthetic data\nAccessing real-world data can sometimes be difficult, whether due to timing, privacy concerns, access problems, lack of participation in healthcare by certain groups, or rarity of a disease.\nSynthetic data (information that’s artificially generated rather than produced by real-world events) can be used if you’re having difficulty accessing real-world datasets. Synthetic data may also satisfy a use case (for example, for data augmentation purposes).\nSynthetic data can be used for prototyping or building analytics dashboards that are ready to plug into real-world datasets. It can also be used for machine learning purposes, including to integrate data relating to under-represented conditions and groups of people, and to protect privacy. Recent publications have overviewed the use of synthetic data (see Chen et al. 2021).\n\nChen, R J, Lu, M Y, Chen, T Y, Williamson, D F K, and Mahmood, F. 2021. “Synthetic data in machine learning for medicine and healthcare”. Nature Biomedical Engineering 5.6, 493–497. https://www.nature.com/articles/s41551-021-00751-8.\nHowever, off-the-shelf models that generate synthetic data (such as Synthea) are not mature and should be carefully assessed for local use. We highly recommend that you’re aware of how the synthetic data is generated and what the limitations are before using it.\nOff-the-shelf calculators can also be considered in lieu of building or enhancing models with synthetic data, with a caveat that these calculators are ideally subject to local validation. There are tools available such as Te Pokapū Hātepe o Aotearoa New Zealand Algorithm Hub which provide a shared knowledge-base of pre-trained models, algorithms and risk calculators, reviewed by a multidisciplinary governance group.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "modelling.html",
    "href": "modelling.html",
    "title": "8  Modelling",
    "section": "",
    "text": "8.1 Keep in mind\nThis guidance is applicable to any analytical technique.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#keep-in-mind",
    "href": "modelling.html#keep-in-mind",
    "title": "8  Modelling",
    "section": "",
    "text": "Apply an equity lens all the way through the model lifecycle: consider who is benefiting from a model, who is vulnerable, and who is excluded. This lens should inform the data chosen, how the data is processed, algorithm choice, how the model is evaluated, and how the model is used\nClosely engage with the stakeholders and data providers (see End-user engagement)\nMaintain clinician engagement throughout\nHave good quality documentation to share the work with others (see Documentation and management)\nVersion control the code base so that work is reproducible (see Documentation and management)\nCheck data quality and availability including operational concerns, sanity checking and bias (see Data quality)\nConsider your audience when presenting data (see Transparency, interpretability and explanation).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#modelling-process",
    "href": "modelling.html#modelling-process",
    "title": "8  Modelling",
    "section": "8.2 Modelling process",
    "text": "8.2 Modelling process\nAlthough details can differ from case to case, there are several general modelling process steps and principles that can help achieve good results.\n\n\n\nModelling process overview, adapted from Complex Systems Modelling Group 2010\n\n\nScoping should clearly define what problem a model is trying to solve. This affects almost all the later steps outlined above. In general, a more specific scope is more likely to lead to success, which should also be defined at the outset. Documenting the scope usually involves sign-off from oversight groups, and can reduce wasted effort or misunderstanding later in the project.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#modelling-approaches",
    "href": "modelling.html#modelling-approaches",
    "title": "8  Modelling",
    "section": "8.3 Modelling approaches",
    "text": "8.3 Modelling approaches\nA general principle of modelling design is to select models that are as simple as possible, but are capable of solving the defined problem. Consider the requirements for ‘explainability’ (the concept that a model and its output can be explained in a way that “makes sense” to a human) for your use case when choosing your model. For example a logistical regression model is easily communicated (see Transparency, Interpretability and Explanation).\n\n\n\n\n\n\nTip\n\n\n\nMultiple modelling approaches may be suitable for solving one problem, with each approach having advantages and disadvantages. If capacity and feasibility allow, it’s often beneficial to use more than one technique. Similar insights provided via different approaches only add to the credibility of modelling.\n\n\nYour choice of technique will be influenced by your ability to explain how it works to end users, and operationalise the inputs to the model. Consider this prior to choosing your model/s.\nBelow is a non-exhaustive list of categories of quantitative models and the example healthcare problems that they are used for:\n\n\n\n\n\n\n\nModel\nExample use case\n\n\n\n\nDescriptive statistics\nUnderstand disease prevalence and demographics of patient cohorts\n\n\nRegression model\nRelationship between risk factors vs. cost of cancer treatment\n\n\nClassification model\nIdentify high risk patients\n\n\nClustering model\nIdentify different patient cohorts\n\n\nSimulations\nWorkforce planning during a pandemic using simulated disease spread trend\n\n\nOptimisation\nMinimising surgeons’ overtime while planning as effectively as possible to meet surgery demands and optimise throughput\n\n\nSystem dynamics\nGP training plan for the next ten years given the population and workforce dynamics\n\n\nLanguage models\nModel-aided auto coding of clinical documents\n\n\n\nThere are numerous additional examples (also see Complex Systems Modelling Group 2010).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#documentation-and-management",
    "href": "modelling.html#documentation-and-management",
    "title": "8  Modelling",
    "section": "8.4 Documentation and management",
    "text": "8.4 Documentation and management\nWe highly recommend that you document all the steps of your project. Information about the software used in model development and the modelling environment itself should be comprehensively documented, including the language or software used, dependencies, and version numbers, to enable the model to be able to be reproduced. This is particularly important as packages are updated often, which can cause a change that affects the production, performance, or other aspects of a model.\nFor machine learning model management, there are open-source tools such as MLflow that can efficiently manage the machine learning lifecycle. This includes experimentation, reproducibility, deployment, and a central model registry. Tools including Docker, pipenv and renv assist portability and reproducibility, and help generate documentation.\nWhen communicating a model to your stakeholders, make sure you explicitly explain the assumptions on which your model has been developed, and the limitations of the model. Documentation should clearly outline the input parameters and ranges, and any implications from sensitivity analyses (see Transparency, Interpretability and explanation).\nTransparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) (Moons et al. 2015) is a well established and practical template used for healthcare model reporting. For simpler reporting, consider Minimum information about clinical artificial intelligence modelling: the MI-CLAIM checklist (Norgeot et al. 2020) as an alternative. The EQUATOR Network, an international initiative promoting high-quality transparent health research, also has useful resources (see reference).\n\nMoons, K G M, Altman, D G, Reitsma, J B, Ioannidis, J P A, Macaskill, P, Steyerberg, E W, Vickers, A J, Ransohoff, D F, and Collins, G S. 2015. “Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): Explanation and elaboration”. Annals of Internal Medicine 162.1, W1–73.\n\nNorgeot, B, Quer, G, Beaulieu-Jones, B K, Torkamani, A, Dias, R, Gianfrancesco, M, Arnaout, R, Kohane, I S, Saria, S, Topol, E, Obermeyer, Z, Yu, B, and Butte, A J. 2020. “Minimum information about clinical artificial intelligence modeling: The MI-CLAIM checklist”. Nature Medicine 26.9, 1320–1324. https://www.nature.com/articles/s41591-020-1041-y.\n\nMō te pokapū - about the hub. https://algorithmhub.co.nz/about.\nFor key information on governance requirements, refer to the governance process and Algorithm Information Request template for the New Zealand Algorithm Hub (Mō te pokapū - about the hub).\nDocuments and/or artefacts commonly related to producing and deploying a model can include:\n\nDesign overview\nAssumptions document\nData dictionary\nExample of consent forms\nEvidence of ethical approval or exemption\nTechnical documentation\nSlide deck for communication\nProspective evaluation.\n\nUsing a version control system such as git is very important for coordinating activity, avoiding rework, auditing - and generally maintaining your own sanity.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#modelling-tools",
    "href": "modelling.html#modelling-tools",
    "title": "8  Modelling",
    "section": "8.5 Modelling tools",
    "text": "8.5 Modelling tools\nCommonly used tools for statistical analysis and modelling that require minimum programming:\n\nStatistical Package for the Social Sciences (SPSS): Suitable for descriptive statistics, parametric and non-parametric analyses, and visualisation of analysis results.\nMicrosoft Excel: Easy to view initial observation over tabular data, and suitable to generate summary metrics of data and create simple data visualisations. Excel has some notable drawbacks though. including:\n\nA column and row size limit\nDifficulty in auditing and rerunning with different data\nEntangled data and analytics layers, which can compromise data integrity.\n\nCloud based modelling platforms such as BigML: These platforms provide a web interface for you to easily run machine learning modelling experiments, as well as interpretability and collaborative features. However, you may need to upload your data to the cloud, which may require additional data policy checking.\n\nCommonly used programming tools for statistical analysis and modelling:\n\nR: A fully open-source software environment for statistical computing and graphics. The RStudio GUI is available, and the R Markdown plug-in can facilitate R code and Markdown-based report/documentation generation, and thus collaboration. The R community is active in developing and maintaining various analysing and modelling packages. Essential packages for data science include:\n\nFor data manipulation: dplyr, tidyr, dbplyr, and other Tidyverse packages\nFor modelling: caret, e1071, mlr3\nFor visualisation: ggplot2, plotly\nFor report generation: knitr\nFor creating interactive web interfaces: shiny\n\nPython: A high-level, object-oriented programming language with good code readability. Python has a comprehensive ecosystem to support common statistical analysis, machine learning, deep learning and the implementation and deployment of these data science features. Jupyter notebook/lab is a widely used tool in the Python ecosystem for shareable data science work. Many dependencies in the Python ecosystem are open-source. Essential Python dependencies for data science include:\n\nFor data manipulation: pandas, numpy\nFor statistical analysis: scipy, statsmodels\nFor modelling including machine learning: scikit-learn, statsmodels\nFor deep learning: tensorflow, pytorch, keras\nFor visualisation: matplotlib, seaborn, plotly, bokeh\nFor creating interactive web interfaces: dash, streamlit\n\nSAS: A statistical software suite for data management, advanced analytics, multivariate analysis, predictive analytics and so on. SAS has its own GUI which is suitable if you’re a non-technical user, but programming via the SAS language can provide more flexibility and functionality in the analysis. It provides free SAS Ondemand for Academics (previous University Edition) for non-commercial users such as students and educators.\nMatlab: A proprietary programming language and environment optimised for numeric computing. Matlab is powerful for linear algebra and is used for applications such as simulation and signal processing. For machine learning and deep learning, Matlab provides the Statistics and Machine Learning Toolbox and the Deep Learning Toolbox, respectively.\n\nOpen source tools such as R and Python are a great place for you to start as they provide transparency, the ability to collaborate, and have a low barrier to entry in terms of cost. However, they may not work in practice in every use case, and we’re not clear on how stable these tools will remain following version changes over time. Commercial tools tend to have a higher cost than their open source counterparts, but offer more controlled and supported experiences.\n\n\n\n\n\n\nTip\n\n\n\nCheck the license terms for the tools you’re using. Models developed using some commercial tools may not be able to be used in a specific deployment environment.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#experiment-tracking-tools",
    "href": "modelling.html#experiment-tracking-tools",
    "title": "8  Modelling",
    "section": "8.6 Experiment tracking tools",
    "text": "8.6 Experiment tracking tools\nModel development is an iterative process and it takes time to find an optimal set of configurations for the problem you are trying to solve. Monitoring experiment configurations and model performance can be cumbersome, especially when the complexity of a model grows. There are many experiment tracking tools available to help you with machine learning model development, such as this list from Neptune.ai.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#model-validation",
    "href": "modelling.html#model-validation",
    "title": "8  Modelling",
    "section": "8.7 Model validation",
    "text": "8.7 Model validation\n\n8.7.1 Data partitioning\nModels are usually tuned to the dataset that they are trained with. Using independent datasets for validating the performance of a model is preferred. However, in the healthcare domain, data sharing can make this difficult.\nWhen independent datasets aren’t available, your model needs to be validated using the development dataset. There are two main approaches for train-test data splitting and model validation.\n\n\n\n\n\n\nNote\n\n\n\nBootstrapping (multiple samples with replacement) can also be used for model development, with models evaluated with an “out of bag” sample. Bootstrap sampling is embedded in boosting and bagging algorithms.\n\n\n\nCross validation\n\nThis method mixes and shuffles all data points and splits them into k folds (usually 5-10) for iterative performance measuring. In each iteration, one fold is left out as the testing set and the others are used as the training set. By these iterations, k measures of the chosen performance metric(s) can be obtained to assess the model’s average performance and variance, which is used as an estimate of the model’s performance on unseen data.\n\n\n\nExample of K-Fold Cross Validation when k=5\n\n\nWhen using cross validation, it’s important you ensure there are no significant data changes across time within the model development dataset. For example, if a policy change was implemented at a date in the middle of your dataset time range - this may impact the data and affect model performance at some time post that date. You will also want to ensure that there are no foreseeable data changes between the development set and the future data that the model would be applied on. In healthcare, it’s also important to check whether the model will be used at an event level (e.g. a hospitalisation event or GP visit) or patient level in advance.\n:::{.callout-tip} There could be multiple events for the same patient in a dataset. If no proper consideration is taken into account while splitting the testing data (or other preprocessing like deduplication), this could potentially lead to information leakage and inaccuracy. :::\nFor example, in a longitudinal dataset of health encounters, the dataset could be split to group all of a patient’s encounters together (so these are not split across sets), or split by time (which could mean a patient’s encounters are split between different validation sets). You should consider whether these types of splitting will make a meaningful difference to your validation exercises. Also be mindful of anything that involves a flow or transfer.\n\nTime-wise validation\n\nMany healthcare model applications have a time dimension, as data is collected by healthcare events that happen across time. There may be a trend in the data as people age, population structure changes, and healthcare technologies and systems evolve.\nTo better estimate the future performance of a model, a time-wise splitting and validating approach should be taken. This method sets data of a specified period along time as the testing set and uses the data before the testing set as training data. A visualisation of performance change over time will be obtained after a number of iterations, which provides an estimation of the trend of the model’s performance change in the near future - whether it will it be relatively stable, gradually decreasing or increasing.\nSome models have a complex hyperparameter space and require an additional split of data for early stop-in training or additional hyperparameter optimisation. In classification models that rely on a cut-off threshold in application (for example, a high-risk patient identification model based on numeric risk scores), the cut-off threshold can also be seen as part of the model hyperparameters, and its determination should be considered as part of the model development. As a result, the performance validation process should be using a partition of data that is not used in either model training, additional hyperparameter optimisation, or the threshold optimisation in particular.\nIn all cases, you should take care to ensure that the sampling unit is complete. For instance, you may be modelling a flow of events, in which case the sampling unit is likely to be a patient. If the modelled data consists of multiple records per patient, it’s important to ensure that complete patient records are sampled.\n\n\n8.7.2 Performance metrics\nTo assess model performance, the type of performance metric you use (as outlined below) will depend on the model being evaluated and the context of the project. For models that predict a value, such as a linear regression model, R-squared and root mean square error (RMSE) are common.\nIn health, metrics calculated from a confusion matrix for models that output a class (e.g. a decision tree) or probability (e.g. logistic regression) are commonly used. To produce a confusion matrix for a model that outputs a probability, a probability threshold is applied (where values above the threshold are positive and values below are negative). The probability threshold can be set to optimise a particular metric, can be set by other optimisation techniques, or otherwise set according to the use case. Seek clinical input on the appropriate thresholds as they can impact patient care.\nThere are a number of common performance metrics that can be used (although it’s important to note that not all are appropriate for each model or data type or question being answered):\n\nPrecision - quantifies the proportion of positive class predictions that actually belong to the positive class\nRecall - quantifies the number of positive class predictions made out of all positive examples in the dataset\nModel accuracy - a machine learning classification model performance metric that’s defined as the ratio of true positives and true negatives to all positive and negative observations\nF-Measure - provides a single score that balances both the concerns of precision and recall in one number\nPositive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure. PPV and NPV are best described as the clinical relevance of a test.\nPrevalence - the number of cases in a defined population at a single point in time and is expressed as a decimal or a percentage\nSensitivity - the percentage of true positives (for example, 90% sensitivity = 90% of people who have the target disease will test positive)\nSpecificity - the percentage of true negatives (for example, 90% specificity = 90% of people who do not have the target disease will test negative)\nAUC-ROC - a performance metric for “discrimination”; it tells you about the model’s ability to discriminate between cases (positive examples) and non-cases (negative examples). For a ranking use case AUC-ROC is a measure that indicates how good the model is at ranking cases based on a score (how likely any two cases are correctly ordered). 0.5 indicates that the model is no better than random at ranking and 1 indicates a perfect model.\nAUC-PRC - a measure that indicates how good the model is at minimising the trade-off between precision and recall. A high AUC-PRC shows that the model can achieve high recall and high precision at the same time.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#useful-resources",
    "href": "modelling.html#useful-resources",
    "title": "8  Modelling",
    "section": "8.8 Useful resources",
    "text": "8.8 Useful resources\n\nRules of Machine Learning: Best Practices for ML Engineering (Google Machine Learning Guides)\nTen rules for the credible practice of modelling nd simulation in healthcare (Erdemir et al. 2020)\nBook on modelling in healthcare (Complex Systems Modelling Group 2010)\nA Practical Guide to Maintaining Machine Learning in Production\nModel cards for model reporting (Mitchell et al. 2019)\n\n\n\n\nErdemir, A, Mulugeta, L, Ku, J P, Drach, A, Horner, M, Morrison, T M, Peng, G C Y, Vadigepalli, R, Lytton, W W, and Myers, J G. 2020. “Credible practice of modeling and simulation in healthcare: Ten rules from a multidisciplinary perspective”. Journal of Translational Medicine 18.1, 369. https://doi.org/10.1186/s12967-020-02540-4.\n\nComplex Systems Modelling Group. 2010. Modelling in healthcare. Providence, Rhode Island. http://www.ams.org/mbk/074.\n\nMitchell, M, Wu, S, Zaldivar, A, Barnes, P, Vasserman, L, Hutchinson, B, Spitzer, E, Raji, I D, and Gebru, T. 2019. “Model cards for model reporting”. in Proceedings of the conference on fairness, accountability, and transparency., 220–229. https://dl.acm.org/doi/10.1145/3287560.3287596.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modelling</span>"
    ]
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "9  Evaluation",
    "section": "",
    "text": "9.1 Model safety\nMaking a model safe for use in a healthcare setting generally involves ensuring that its rates of falsely classifying results as positive or negative are low.\nWhen models are implemented, often errors in classifying high-risk individuals can be identified because there is a follow-up step associated with that high risk which aims to reduce the risk. However, errors in classifying low-risk individuals often have two possible explanations:\nIt is usually difficult for a data scientist to know what additional data could have been used. It’s helpful for your clinical lead to audit a number of false negative results to determine if the outcome is truly unpredictable or if additional data will help with the prediction.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "evaluation.html#model-safety",
    "href": "evaluation.html#model-safety",
    "title": "9  Evaluation",
    "section": "",
    "text": "Note\n\n\n\nFrom a clinical perspective, the main area of concern is often false negative or low-risk individuals with a positive outcome. You should include clinical experts in discussions on appropriate model thresholds to address this.\n\n\n\n\nThe outcome was unpredictable\nUsing additional data could improve the accuracy of the model’s prediction",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "evaluation.html#bias-in-data",
    "href": "evaluation.html#bias-in-data",
    "title": "9  Evaluation",
    "section": "9.2 Bias in data",
    "text": "9.2 Bias in data\nData is collected in a particular context, which leads to bias. This can come from different sources including historical bias, data imbalance, missingness, and human prejudice. There’s no single best definition of bias or fairness that applies equally well for every data science application.\nWhile training machine learning models using historically collected data, or drawing any conclusion from data, you should be typically mindful about the potential bias in the data, regarding sensitive attributes such as age, ethnicity and gender. Bias-related harms can be reinforced by machine learning models and systems.\nThe Manatū Hauora Ministry of Health’s Emerging Health Technology Introductory Guidance (PDF) section on Bias is a good place to start learning.\nMachine learning fairness is a broad topic; relevant reading includes a section in Google’s crash course in machine learning, its reference on A Marketer’s Guide to Machine Learning Fairness, and AI Fairness 360 (Bellamy et al. 2018).\n\nBellamy, R K E, Dey, K, Hind, M, Hoffman, S C, Houde, S, Kannan, K, Lohia, P, Martino, J, Mehta, S, Mojsilovic, A, Nagar, S, Ramamurthy, K N, Richards, J, Saha, D, Sattigeri, P, Singh, M, Varshney, K R, and Zhang, Y. 2018. “AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias”. http://arxiv.org/abs/1810.01943.\nDue to the historical focus of cohort studies, certain groups of the population - such as certain ethnic groups or females - may be underrepresented and more vulnerable to bias in studies where any conclusions were drawn or models were trained.\nIn evaluation work, it’s important to measure the goodness of fit, accuracy and other metrics of a model from multiple perspectives rather than the overall metrics only. Consider the basic measurement aspects with respect to sensitive attributes (for example gender and ethnicity):\n\nThe difference of actual patient data metrics stratified by sensitive attributes - whether there’s any inequity among the stratified groups, and what bias it may bring into the models\nThe difference of predicted outcome metrics stratified by sensitive attributes - whether there is any inequity in model predictions among the stratified groups, and what downstream consequence this may cause\nThe difference of model performance metrics among the stratified groups - whether the model is treating the groups equally, and what downstream consequence this may cause.\n\nReporting metrics with stratification by sensitive attributes whenever applicable can help to more easily maintain an equity lens. Performance of the IBIS/Tyrer-Cuzick model of breast cancer risk by race and ethnicity in the Women’s Health Initiative is an example see Kurian et al. 2021.\n\nKurian, A W, Hughes, E, Simmons, T, Bernhisel, R, Probst, B, Meek, S, Caswell‐Jin, J L, John, E M, Lanchbury, J S, Slavin, T P, Wagner, S, Gutin, A, Rohan, T E, Shadyab, A H, Manson, J E, Lane, D, Chlebowski, R T, and Stefanick, M L. 2021. “Performance of the IBIS/tyrer‐cuzick model of breast cancer risk by race and ethnicity in the women’s health initiative”. Cancer 127.20, 3742–3750. https://onlinelibrary.wiley.com/doi/10.1002/cncr.33767.\nThere’s a trade-off between data informativeness, model performance and fairness. Most bias mitigation methods can’t avoid playing with this balance. We highly recommend taking into account the use case and follow-up impacts while deciding which bias mitigation method to be used, and how it’s used.\nMitigating bias and improving fairness is mostly not a technical challenge but a much broader systematic challenge. We recommend including diverse voices and perspectives in data science work by, for example, having Māori researchers involved in your project.\n\n\n\n\n\n\nNote\n\n\n\nEven if no mitigation can be included, we recommend that the bias itself should be analysed and reported if possible, especially that it is important to identify who is most vulnerable to the bias-related harms.\n\n\n\n9.2.1 Useful tools and resources\n\nAequitas\nAI Fairness 360 (IBM)\nAudit-AI\nFairlearn (Microsoft)\nFairness Indicator (Google)\nFATE: Fairness, Accountability, Transparency, and Ethics in AI (Microsoft)\nThe LinkedIn Fairness Toolkit (LiFT)\nML-fairness-gym (Google)\nPROBAST (see Wolff et al. 2019 and Moons et al. 2019)\n\n\nWolff, R F, Moons, K G M, Riley, R D, Whiting, P F, Westwood, M, Collins, G S, Reitsma, J B, Kleijnen, J, and Mallett, S. 2019. “PROBAST: A tool to assess the risk of bias and applicability of prediction model studies”. Annals of Internal Medicine 170.1, 51–58. https://www.acpjournals.org/doi/10.7326/M18-1376.\n\nMoons, K G M, Wolff, R F, Riley, R D, Whiting, P F, Westwood, M, Collins, G S, Reitsma, J B, Kleijnen, J, and Mallett, S. 2019. “PROBAST: A tool to assess risk of bias and applicability of prediction model studies: Explanation and elaboration”. Annals of Internal Medicine 170.1, W1. http://annals.org/article.aspx?doi=10.7326/M18-1377.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "evaluation.html#keep-subject-matter-experts-involved-throughout",
    "href": "evaluation.html#keep-subject-matter-experts-involved-throughout",
    "title": "9  Evaluation",
    "section": "9.3 Keep subject matter experts involved throughout",
    "text": "9.3 Keep subject matter experts involved throughout\nIt’s very important to continually engage subject matter experts, such as clinicians, throughout your project. This can encompass involving clinicians in the project as an advisory level, and can be extended by involving clinicians in iterative development of models, such as providing labels.\n\n\n\n\n\n\nTip\n\n\n\nIt’s incredibly important to keep the perspectives of subject matter experts front of mind throughout your project. If these experts are not involved in the data science process, their acceptance of the outputs will be limited or absent. This may mean your model isn’t able to be used to its full potential - or in the worst case scenario, not used at all.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "evaluation.html#transparency-interpretability-and-explanation",
    "href": "evaluation.html#transparency-interpretability-and-explanation",
    "title": "9  Evaluation",
    "section": "9.4 Transparency, interpretability, and explanation",
    "text": "9.4 Transparency, interpretability, and explanation\nIn the context of health, it’s especially important to communicate findings in a way that builds trust in your model development process and outputs. If trust isn’t built in your model, it may not be adopted in practices. However, while robust and transparent explainability may seem like a good goal for an AI model to become useful, there may be some disadvantages to this approach (Babic et al. 2021).\n\nBabic, B, Gerke, S, Evgeniou, T, and Cohen, I G. 2021. “Beware explanations from AI in health care”. Science 373.6552, 284–286. https://www.science.org/doi/abs/10.1126/science.abg1834.\nThe requirement for models to be transparent, interpretable and explainable should guide your early modelling decisions, such as which algorithm to use and how to treat inputs. Linear models tend to be more interpretable and explainable; so too are models that are built with inputs that haven’t been subject to significant transformations or weightings.\nThe choice of a ‘simpler’ model may compromise model performance, meaning better outcomes are sacrificed for the sake of explainability. There is little point however in deploying a deep learning model that has excellent performance, but isn’t trusted or used. You should carefully consider this trade-off.\nYou should also consider outcome measures that are tangible and meaningful to your end user, and that have some relationship to the project goal, such as hospitalisation, mortality, or rankings.\nFinally, you should indicate which inputs are most important to the model performance or model outputs (feature importance) and how the model inputs are weighted in relation to model outputs, and what that means in practice. For example, if a model includes modifiable inputs, a model user will want to know how changes to that input might affect an outcome. From an equity perspective, a model user will also want to know to what extent inputs such as ethnicity, age, gender or deprivation impact the outcome.\nThese considerations are also relevant to governance of models. GDPR’s regulation specifically emphasises a model’s transparency, accountability and governance (see Kaminski and Malgieri 2021).\n\nKaminski, M E and Malgieri, G. 2021. “Algorithmic impact assessments under the GDPR: Producing multi-layered explanations”. International Data Privacy Law 11.2, 125–144. https://academic.oup.com/idpl/article/11/2/125/6024963.\n\n9.4.1 Transparency around the inputs\nIt’s important to provide good data definitions and reasons for how data has been treated. For example:\n\n“The model includes age but it has been grouped into 3 categories (18-39, 40-59 and 60+) to simplify the model and handle outliers without compromising performance”; or\n“The count of regular medications includes medications that have been prescribed in the two years prior to the test positive date for this infection. Regular medications are medications that have been prescribed at least four times over that period. Prescription data rather than dispensing data is used as it has better coverage.”\n\nProviding the provenance of data is also important. For example:\n\n“This data came from a national collection of data that went through a quality assurance process. It is current and relevant to the problem being answered in this way…”\n\n\n\n9.4.2 Interpretation of the output\nThere are different types of output from different algorithms, such as risk scores, predictions, and simulation results. In particular, users of your model need to understand what the value means in the context of how it will be used.\nFor example, depending on how the model is built and the use case, a risk score of 0.8 might mean 80% probability of an outcome, or it may be a value that only makes sense while compared with other people’s scores in a cohort. If the risk score has been developed for a ranking use case, your end user needs to understand that the output for a given person has meaning in relation to outputs for other people to determine who is at higher risk, rather than as a standalone value.\n\n\n9.4.3 Transparency of algorithm development\nYou should audit the behaviour of an algorithm at the population level. Unexpected behaviours indicate defects or limitations of the model, and are roadblocks for gaining trust. For example, the relationship between the predicted values and certain covariants (e.g. increased predicted mortality risk vs. age) for the validation cohort should match with empirical evidence, or the clinician’s judgement. Interpretation techniques such as partial dependence plots can facilitate such investigation.\nYou should also explain model prediction for specific individuals. For example, a person’s readmission risk score can be attributed to age, previous hospitalisation history, cancer diagnosis, ethnicity and other risk factors. Local surrogate models or Shapley values based methods are commonly adopted techniques to provide explanations at the individual level. Interpretable Machine Learning (Christoph Molnar 2022) provides more details about model interpretation techniques.\n\nChristoph Molnar. 2022. “Interpretable machine learning”. https://christophm.github.io/interpretable-ml-book/index.html.\n\n\n\n\n\n\nTip\n\n\n\nClinicians or other stakeholders need to be involved in the population- and individual-level algorithm auditing, driving iterations of algorithm development with their feedback.\n\n\nWhere possible, making the code base and dataset public adds credibility as others can understand the data, how the model was developed, allowing them to replicate findings themselves and to challenge development assumptions, or even suggest improvements.\n\n\n9.4.4 Understanding the performance of an algorithm\nDefining performance metrics for your algorithm in the context of the problem it’s trying to solve is important.\nTake, for example, a use case where a model is being used to predict a condition (with prevalence 2%) that requires an intervention. A positive predictive value of 0.95 means that of those people that presented with the condition, 95% were identified by the model. Putting this into context, we can say that if 1000 people a day were assessed, we would expect that 20 of them would require the intervention. The model would identify 19 of those people, meaning weekly, 5 people with that condition would be missed if we were to rely solely on model outputs.\nInformation like this can help a clinician understand that while the model performs well, in practice they might like to supplement model outputs with other assessments.\nFor models that output probabilities, such as logistic regression, such analyses can be useful in helping clinicians quantify and understand the trade-off between false positives and false negatives in order to decide which decision thresholds may be appropriate.\n\n\n\n\n\n\nTip\n\n\n\nPlain language and accessible explanations not only help build trust in the model; they help the user understand how to use the model, which helps it to be adopted.\n\n\n\n\n9.4.5 Understanding the impact of an algorithm\n\n\n\n\n\n\nTip\n\n\n\nYou should evaluate your algorithm’s benefit and cost against realistic scenarios. Classic model performance metrics such RMSE, accuracy, precision or recall generally won’t go far enough to illustrate the consequences of integrating the algorithm into a healthcare workflow.\n\n\nHow your algorithm can be integrated into a workflow needs to be understood and well documented. It’s worth further evaluating what the potential healthcare outcome could be, especially when there’s a limit to clinical capacity.\nHere’s an example: A model is developed to classify GP-referred patients into high priority and low priority using a certain priority score threshold, in order to deliver timely triaging. However, there are already many people waiting in the triage queue, people newly referred each day, and the number of referral requests clinicians can process has a limit and uncertainty. By looking at the classification metrics of this model, without knowing how and at which step of the process the model will be used, it’s hard to tell exactly whether the integration of the model will bring more benefit than cost in the waiting time for patients who are in urgent need.\nDefining proper impact metrics that take the use case and goal of your modelling into consideration, and carefully running through a further evaluation against workflow, will provide more informative insights than classic model performance metrics alone.\nDeterministic or stochastic simulation techniques can be used for this evaluation when a working scenario can be quantitatively described. An example of this is the New Zealand business case for a hospital avoidance programme using a readmission risk model, which presented its financial impact in healthcare (Vaithianathan et al. 2012).\n\n\n\nVaithianathan, R, Jiang, N, and Ashton, T. 2012. “A model for predicting readmission risk in new zealand”. http://hdl.handle.net/10419/242509.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "deployment-lifecycle.html",
    "href": "deployment-lifecycle.html",
    "title": "10  Deployment & lifecycle maintenance",
    "section": "",
    "text": "10.1 Operational deployment\nHealth data science projects often aim to produce tools that support decision making, through presenting insights or suggestions. Considering the implications of the intended use of your algorithm in practice is critical to success because if the model is used outside of its original intended use case, the subsequent decisions made based on the outputs may be invalid.\nWe recommend planning for this from the start of your project if operational deployment is one of your explicit goals, as there is a big difference between research and operationalising something. This area is sometimes called MLOps or ModelOps. Engage early with deployment experts, and involve the right mix of expertise for a successful implementation (see Keep subject matter experts involved throughout).\nSome questions worth considering include:\nThe list below outlines ideas and recommendations for operationally deploying machine learning solutions within the medical sector:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deployment & lifecycle maintenance</span>"
    ]
  },
  {
    "objectID": "deployment-lifecycle.html#operational-deployment",
    "href": "deployment-lifecycle.html#operational-deployment",
    "title": "10  Deployment & lifecycle maintenance",
    "section": "",
    "text": "Who are the users, and what knowledge, capability, and/or training will they need?\nIs all the relevant data available at the point it is intended to be used, and is it current?\nIs it feasible to access ongoing operational data supply to support the model?\nHow will models, decisions and/or insights be presented to the user?\nCan users understand how the model came to a conclusion and what action is required? (see Transparency, interpretability, and explanation)\nWhat monitoring, IT security and safeguards are necessary?\nWhat systems will be utilised, and who is responsible for them?\nWho will provide governance oversight of the model?\nWill the model be updated when new data becomes available?\nWhat exactly is expected to be deployed? An API? A frontend?\nHow are the models and API going to be versioned?\nHow much traffic are you expecting?\nHow much response time and performance in time is acceptable?\nWhat level of work is required from warehouse developers when developing models? A simple model is more likely to be implemented than one which is complicated even if there is a performance decrease.\n\n\n\nDraw clear lines between research, analytics and software development. If you’re looking to offer software as a service (SaaS), you’ll need software engineers who are familiar with software development practices (e.g. writing production-grade code, developing infrastructure)\nKeep things simple: most prospective customers in the medical sector are conservative and need to understand (at least to some extent) solutions that are being offered to them. They’re also probably using outdated solutions (as opposed to cutting edge research)\nMany prospective customers in the medical sector will be unwilling to send their data to an API over the public internet. You may need to specifically focus on the niche group of customers who don’t have these constraints, or you’ll need to develop solutions that can be deployed on premise. This has profound implications on your architectural decisions (e.g. federated learning), your ability to provide maintenance and support as well as your ability to keep tight control over your source code\nIdentifiable health data is regulated around the world. You need to ensure that you follow the right security practices for your clients to be able to use your services. This also has downstream implications on debugging, logging, and performance tracking\nMuch of the publicly-available health data that you may wish to deploy in production to train models have licences that forbid commercial usage – make sure you check these constraints before committing to a project\nIf you’re offering machine learning models to external clients which leverage open source data, you don’t need to regularly retrain these models, as you’ll most likely be using a static snapshot for a one-off training exercise. This simplifies model management\nFor IT implementation at partner and/or clinical sites, typically design, governance, and security sign-off will be required before people can be allocated to the work. This requires clear articulation of the benefits of the work in order to be prioritised. Ensure there is strong clinical engagement from the partner site to drive this (see End-user Engagement and collaboration)\nAlso refer to the Ministry of Health - Emerging Health Technology Advice & Guidance on operationalising algorithms",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deployment & lifecycle maintenance</span>"
    ]
  },
  {
    "objectID": "deployment-lifecycle.html#lifecycle-model-maintenance",
    "href": "deployment-lifecycle.html#lifecycle-model-maintenance",
    "title": "10  Deployment & lifecycle maintenance",
    "section": "10.2 Lifecycle model maintenance",
    "text": "10.2 Lifecycle model maintenance\n\n\n\nThe CRISP-DM Data Science Lifecycle (Shearer 2000)\nShearer, C. 2000. “The CRISP-DM model: The new blueprint for data mining”. Journal of Data Warehousing 5.4, 13–22.\n\n\n\nDeploying a model is never the end of modelling work. During its continuing life cycle, the model and its working environment need to be overseen continuously. Some early questions to consider are:\n\nWhat’s the governance process?\nWho’s responsible for oversight and/or maintenance?\nWhat’s the process for monitoring model drift?\nIs the model being used in the way it was originally intended, as outlined in the original use case?\nDo the intended users of the model understand, trust and use the model outputs?\n\nIn the case of “model drift” (which occurs when non-negligible performance decrease is detected, or any condition for the model to properly work is no longer satisfied), the model and model application need to be reviewed. Where necessary, the partial or whole modelling process should be reapplied to refresh the model.\nRefer also to Transparency, interpretability, and explanation.\n\n10.2.1 Model refreshment\nThere are two main approaches for triggering model refreshment:\n\nTime based refreshment: Retrain the model (regardless of the performance) at a regular interval. Better quality data, more current data, data with better population coverage or new data sources may become available over time. This means that a model trained on updated data would likely have significant performance improvements over the existing model. A good understanding of how frequently the data changes is needed for this approach.\nPerformance-based refreshment: Continuously monitor a set of the model performance metrics (and/or other metrics such as bias) to determine when the model needs retraining. A good selection of the panel of metrics and thresholds is needed for this approach.\n\nThere are other considerations for model refreshment, including:\n\nAny change in how the model is being used: Are the users of the model still using it according to the original use case? If the model is being used for a different purpose, is it appropriate for that purpose?\nPotential for increased scrutiny of the model: You don’t want your model to be viewed negatively in public. You should consider how use of the model would look on the front page of the newspaper, particularly if there have been shifts in the inputs, in the outcomes, in the environment (such as new disease variants) or in attitudes that affect social licence.\n\n\n\n10.2.2 Model monitoring\nKey areas to be monitored during model use include:\n\nThe data feed: Are there any changes in the statistical properties of the data that are fed into the model, and the actual data labels or patient outcomes (there will be a lag to collect these) that the model predicts? Are they still similar to the data that the model was trained on? Has a “concept drift” detector been set up in the monitoring process? How is the bias in the data changing?\nThe model performance metrics and bias metrics: Is there any significant drop in the model performance or model fairness and what are the possible reasons to be taken into account for retraining? This could be when a given metric, such as accuracy or combination of metrics, such a sensitivity and specificity have dropped below a certain threshold, or there is a clear downward trend in those metrics.\nThe key assumptions that the modelling was based on: For example, with regard to COVID-19 pandemic modelling, is the current dominating virus variant still the same as the one when the modelling data were collected?\nThe workflow where the model is integrated: Are there any changes in the upstream or downstream steps of the workflow that may make the model less relevant?\nThe risk, benefit and cost of model usage: Does the benefit outweigh the risk in real use? Is the model use case as cost-effective as it was expected at design?\n\nFor a more comprehensive reference of model monitoring and retraining techniques, refer to A Practical Guide to Maintaining Machine Learning in Production.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Deployment & lifecycle maintenance</span>"
    ]
  }
]